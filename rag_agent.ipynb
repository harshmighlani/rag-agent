import os
from dotenv import load_dotenv
load_dotenv()

# ------------------------------------------------
# IMPORTS
# ------------------------------------------------
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from sentence_transformers import SentenceTransformer
from langchain_community.llms import Groq
from langchain.schema import Document
from chromadb.config import Settings

# ------------------------------------------------
# 1. EMBEDDING MANAGER (BAAI BGE-SMALL)
# ------------------------------------------------
class BAAIEmbeddingManager:
    def __init__(self, model_name="BAAI/bge-small-en-v1.5"):
        print(f"Loading embedding model: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.dim = self.model.get_sentence_embedding_dimension()
        print(f"Embedding dimension = {self.dim}")

    def embed_documents(self, texts):
        return self.model.encode(texts, normalize_embeddings=True).tolist()

    def embed_query(self, text):
        return self.model.encode([text], normalize_embeddings=True).tolist()[0]


embedding_manager = BAAIEmbeddingManager()


# ------------------------------------------------
# 2. LOAD PDFS FROM CURRENT DIRECTORY
# ------------------------------------------------
def load_pdfs():
    documents = []
    for file in os.listdir("."):
        if file.lower().endswith(".pdf"):
            loader = PyPDFLoader(file)
            docs = loader.load()
            documents.extend(docs)
    return documents


raw_docs = load_pdfs()
print(f"Loaded {len(raw_docs)} pages from PDFs")


# ------------------------------------------------
# 3. CHUNKING
# ------------------------------------------------
splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=100
)
chunks = splitter.split_documents(raw_docs)
print(f"Created {len(chunks)} chunks")


# ------------------------------------------------
# 4. INIT LOCAL CHROMA (SQLite backend)
# ------------------------------------------------
persist_dir = "./rag_sqlite_db"

settings = Settings(
    chroma_db_impl="sqlite",
    persist_directory=persist_dir
)

vectorstore = Chroma(
    collection_name="rag_pdf_collection",
    embedding_function=embedding_manager,
    client_settings=settings,
)


# ------------------------------------------------
# 5. ADD DOCUMENTS IF EMPTY
# ------------------------------------------------
existing = vectorstore._collection.count()

if existing == 0:
    print("Embedding and storing documents...")
    vectorstore.add_documents(chunks)
    print("Done storing!")
else:
    print(f"Vector DB already has {existing} documents")


# ------------------------------------------------
# 6. INIT LLM (Groq LLaMA 3.1)
# ------------------------------------------------
llm = Groq(
    model="llama-3.1-8b-instant",
    api_key=os.getenv("GROQ_API_KEY")
)


# ------------------------------------------------
# 7. RETRIEVAL + GENERATION FUNCTION
# ------------------------------------------------
def ask(question: str):
    print("\nðŸ’¬ QUESTION:", question)

    retrieved_docs = vectorstore.similarity_search(question, k=4)

    context = "\n\n".join([doc.page_content for doc in retrieved_docs])

    prompt = f"""
You are a helpful RAG agent.

Use ONLY the following context to answer the question.
If the answer is not in the context, say you don't know.

### CONTEXT:
{context}

### QUESTION:
{question}

### ANSWER:
"""

    response = llm.invoke(prompt)
    return response


# ------------------------------------------------
# 8. INTERACTIVE AGENT LOOP
# ------------------------------------------------
while True:
    q = input("\nAsk a question (or 'exit'): ")
    if q.lower() == "exit":
        break
    answer = ask(q)
    print("\nðŸ¤– Answer:\n", answer)
