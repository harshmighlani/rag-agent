# rag-agent

This project implements a complete Retrieval-Augmented Generation (RAG) system using:

ChromaDB (SQLite backend) for vector storage

BAAI/bge-small-en-v1.5 for high-performance, free sentence embeddings

Groq LLaMA-3.1 (8B Instant) for fast, accurate responses

LangChain (community components) for loaders, chunking, and retrieval

The agent loads local PDFs, chunks them, embeds the chunks, stores them into ChromaDB, and answers user questions using retrieval + LLM.

ðŸš€ Features

âœ… Load all PDFs from the current directory 

âœ… Chunk documents using LangChainâ€™s recursive splitter

âœ… Generate 1024-dim embeddings using BAAI

âœ… Store vectors in a local ChromaDB SQLite database

âœ… Retrieve relevant chunks using similarity search

âœ… Answer questions via Groqâ€™s LLaMA-3.1 model

âœ… Fully interactive questionâ†’answer agent loop

ðŸ“‚ Project Structure
project/
â”‚â”€â”€ rag_agent.py           # Main RAG pipeline script

â”‚â”€â”€ .env                   # Contains GROQ_API_KEY

â”‚â”€â”€ sample.pdf             # Your documents (any PDFs placed here are loaded)

â”‚â”€â”€ rag_sqlite_db/         # ChromaDB SQLite vector store (created on first run)

ðŸ§© Requirements

Python 3.10+ (tested on Python 3.13)

A Groq API Key â†’ https://console.groq.com/keys
